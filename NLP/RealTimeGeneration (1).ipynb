{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RealTimeGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4jxr_DBvoO1"
      },
      "source": [
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import LambdaCallback\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "from geopy.geocoders import Nominatim\n",
        "from twython import TwythonStreamer\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Model\n",
        "from textblob import TextBlob \n",
        "from itertools import product\n",
        "from twython import Twython\n",
        "\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import time\n",
        "import nltk\n",
        "import sys\n",
        "import re\n",
        "import io\n"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgWZ97lhd6sj",
        "outputId": "64244b22-477a-4770-f3a3-fd8e5a10501e"
      },
      "source": [
        "pip install twython\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: twython in /usr/local/lib/python3.7/dist-packages (3.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMXhNqa7BwO1",
        "outputId": "8be3c4e1-3f6f-4d46-db19-f125b3a47a97"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlA6uIjBwE1G"
      },
      "source": [
        "class cleaningData:\n",
        "    \n",
        "    def __init__(self, noise_chars):\n",
        "        self.noise_chars = noise_chars \n",
        "    \n",
        "    def removeNoise(self, s):\n",
        "\n",
        "        s = s.lower()  \n",
        "\n",
        "        s = re.sub('@[A-Za-z0-9]+', '', s)\n",
        "        s = re.sub('pleas', 'please', s)\n",
        "        s = re.sub('&[a-zA-Z]+;', '', s)\n",
        "        s = re.sub(\"â€\\x9d&lt;\", \"\", s)\n",
        "        s = re.sub('dont', 'do not', s)\n",
        "        s = re.sub('http\\S+', '', s)\n",
        "        s = re.sub(\"\\\\xa0·\", \" \", s)\n",
        "        s = re.sub('www\\S+', '', s)\n",
        "        s = re.sub(\"(UTC)\", \" \", s)\n",
        "        s = re.sub('^b\\s+', '', s)\n",
        "        s = re.sub(\"â€œ:\", \"\", s)\n",
        "        s = re.sub('<.*?>', '', s)\n",
        "        \n",
        "        for c in self.noise_chars:\n",
        "            s = s.replace(c, '') \n",
        "\n",
        "        clean = s\n",
        "\n",
        "        return clean\n",
        "    \n",
        "    \n",
        "    def removeStopwords(self, s):\n",
        "\n",
        "        words = []\n",
        "\n",
        "        for char in s.split(\" \"):\n",
        "\n",
        "          words.append(char) \n",
        "\n",
        "\n",
        "        SW = stopwords.words('english') \n",
        "\n",
        "        SW.extend(['that','thats',\n",
        "                    'oh', 'aww', 'mr',\n",
        "                    'r', 'what', 'etc',\n",
        "                    'hey', 'within', 'foi',\n",
        "                    'yeah', 'www', 'wa', \n",
        "                    'em', 'am', 'i', 'me',\n",
        "                    'dialmformurderjpg' ])\n",
        "\n",
        "        \n",
        "        cln = [w for w in words if w not in SW] \n",
        "\n",
        "        clnSW = \" \".join(cln)\n",
        "        \n",
        "        return clnSW\n",
        "    \n",
        "    def textNormalization(self, string):\n",
        "\n",
        "        normalized = []\n",
        "        tokenizer = nltk.tokenize.TweetTokenizer() \n",
        "        len_reduced = tokenizer.tokenize(string)\n",
        "        for word in len_reduced:\n",
        "            check_spell = TextBlob(word)\n",
        "            normalized.append(str(check_spell.correct())) \n",
        "        normalized = \" \".join(normalized)\n",
        "        return normalized\n",
        "    \n",
        "    def stemWords(self, string):\n",
        "\n",
        "        words = []\n",
        "        stemmed = []\n",
        "        for i in string.split(\" \"):\n",
        "            words.append(i)\n",
        "        stemmer = PorterStemmer() \n",
        "        stemmed_words = [stemmer.stem(w) for w in words] \n",
        "        for word in stemmed_words:\n",
        "            check_spell = TextBlob(word) \n",
        "            stemmed.append(str(check_spell.correct()))\n",
        "        stemmed = \" \".join(stemmed)\n",
        "        return stemmed\n",
        "    \n",
        "\n",
        "    \n",
        "    def wordTokenize(self, string):\n",
        "\n",
        "        regex = \"[a-zA-Z]+\" \n",
        "        tokenized = re.findall(regex, string) \n",
        "        return tokenized\n",
        "\n",
        "    \n",
        "    def process(self, s):\n",
        "\n",
        "        clean_1 = self.removeNoise(s)\n",
        "        \n",
        "        cSW = self.removeStopwords(clean_1)\n",
        "        cNor = self.textNormalization(cSW)\n",
        "        cStemmed = self.stemWords(cNor)\n",
        "\n",
        "        tokenized = \" \".join(self.wordTokenize(cStemmed))\n",
        "        text_ready = self.removeNoise(tokenized)\n",
        "        preprocessed = self.removeStopwords(text_ready)\n",
        "\n",
        "        return preprocessed"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qvPVPFEwIdB"
      },
      "source": [
        "class preData:\n",
        "  def __init__(self, pre_processor = None, max_features=20000, maxlen=100):\n",
        "    self.preProcessor = pre_processor if pre_processor else PreProcessor(\"#@,.?!¬-\\''=()\") \n",
        "    self.max_features = max_features\n",
        "    self.maxlen = maxlen\n",
        "\n",
        "  def prepare_data(self, X):\n",
        "    \"\"\"\n",
        "    This function prepares the data by performing preprocessing and vectorization.\n",
        "    \"\"\"\n",
        "    try: #try if the data is more than 1 record\n",
        "      preprocessed = []\n",
        "      for comment in X:\n",
        "          preprocessed.append(self.preProcessor.process(comment))\n",
        "      pickle.dump(preprocessed, open('preprocessed_data.pickle','wb'))\n",
        "      data_prepared = self.vectorize(preprocessed)\n",
        "    except: #do if it is only 1 record\n",
        "      preprocessed = self.preProcessor.process(X)\n",
        "      data_prepared = self.vectorize(preprocessed)\n",
        "\n",
        "    return data_prepared\n",
        "\n",
        "  def vectorize(self, X):\n",
        "    \"\"\"\n",
        "    This function vectorizes the preprocessed data.\n",
        "    \"\"\"\n",
        "    list_sentences = X\n",
        "    tokenizer = text.Tokenizer(num_words=self.max_features)\n",
        "    tokenizer.fit_on_texts(list(list_sentences))\n",
        "    list_tokenized = tokenizer.texts_to_sequences(list_sentences) \n",
        "    X_vector = sequence.pad_sequences(list_tokenized, maxlen=self.maxlen) \n",
        "    \n",
        "    return X_vector\n",
        "\n",
        "  def get_model(self):\n",
        "\n",
        "    \n",
        "    embed_size = 128\n",
        "    inp = Input(shape=(self.maxlen, ))\n",
        "    x = Embedding(self.max_features, embed_size)(inp) \n",
        "    x = Bidirectional(LSTM(50, return_sequences=True))(x) \n",
        "    x = GlobalMaxPool1D()(x) \n",
        "    x = Dropout(0.1)(x) \n",
        "    x = Dense(50, activation=\"relu\")(x)   \n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x) \n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "    X_vector = X\n",
        "    self.model = self.get_model()\n",
        "    self.model.load_weights('weights_base.best.hdf5') \n",
        "    return self.model.predict(X_vector)\n",
        "\n",
        "\n",
        "  def evaluate(self, X, y):\n",
        "\n",
        "    X_vector = X\n",
        "    self.model = self.get_model()\n",
        "    self.model.load_weights('weights_base.best.hdf5')\n",
        "    loss, acc = self.model.evaluate(X_vector, y, verbose=2)\n",
        "    print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n",
        "\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc7u7lAPYqGM"
      },
      "source": [
        "class Generative:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    def get_info(self, text):\n",
        "        text = text\n",
        "        maxlen = 40\n",
        "        step = 3\n",
        "        sentences = []\n",
        "        for i in range(0, len(text) - maxlen, step):\n",
        "            sentences.append(text[i: i + maxlen])\n",
        "        chars = pickle.load(open('chars.pickle','rb'))   \n",
        "        char_indices = pickle.load(open('char_indices.pickle','rb'))\n",
        "        indices_char = pickle.load(open('indices_char.pickle','rb'))\n",
        "        return text, chars, char_indices, indices_char, maxlen, sentences\n",
        "  \n",
        "    def sample(self, preds, temperature=0.2):\n",
        "\n",
        "        preds = np.asarray(preds).astype('float64')\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        return np.argmax(probas)\n",
        "\n",
        "    \n",
        "    def generate_tweets(self, corpus, char_to_idx, idx_to_char, chars, maxlen, n_tweets=10, verbose=1):\n",
        "        self.model = load_model('GenerativeModel_compiled_v2')\n",
        "        self.model.load_weights('weights_E_v2.hdf5')\n",
        "        global tweets\n",
        "        tweets = []\n",
        "        \n",
        "        for i in range(1, n_tweets + 1):\n",
        "            begin = random.randint(0, len(corpus) - maxlen - 1)\n",
        "            tweet = u''\n",
        "            sequence = corpus[begin:begin + maxlen]\n",
        "            tweet += sequence\n",
        "            if verbose:\n",
        "                print('Tweet no. %03d' % i)\n",
        "                print('=' * 13)\n",
        "                print('Generating with seed:')\n",
        "                print(sequence)\n",
        "                print('_' * len(sequence))\n",
        "            for _ in range(100):\n",
        "                x = np.zeros((1, maxlen, len(chars)))\n",
        "                for t, char in enumerate(sequence):\n",
        "                    x[0, t, char_to_idx[char]] = 1.0\n",
        "\n",
        "                preds = self.model.predict(x, verbose=0)[0]\n",
        "                next_idx = generative.sample(preds)\n",
        "                next_char = idx_to_char[next_idx]\n",
        "\n",
        "                tweet += next_char\n",
        "                sequence = sequence[1:] + next_char\n",
        "            if verbose:\n",
        "                print(tweet)\n",
        "                print()\n",
        "            tweets.append(tweet)\n",
        "        \n",
        "        twitter.update_status(status=tweets[random.randrange(10)])\n",
        "\n",
        "       \n",
        "        return tweets"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g96Rb-Svy1i"
      },
      "source": [
        "APIKey= 'xug39dkpJteA2AUVzoo97mxD7'\n",
        "\n",
        "APIKeySecret= 'I8Ufoblomatr9Gr7EwE7PtlI35Of1UZ4azOfQ8ZnHb0aPWmRqO'\n",
        "\n",
        "AccesToken= '3542954414-RECdUr9wABHdZGlp9gBmtcfsaSnquwGXpbPC46Z'\n",
        "\n",
        "AccessTokenSecret= 'dylU2sOub2dtYydoGRb9NKkzUrJVuPJC5PVsXioMIPop5'\n"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irotw2epwPqj"
      },
      "source": [
        "cleanData = cleaningData(\"#@,.?!¬-\\''=()\")\n",
        "\n",
        "pre_data = preData()\n",
        "\n",
        "generative = Generative()"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNc9-TRv06g"
      },
      "source": [
        "class MyStreamer(TwythonStreamer):\n",
        "  \n",
        "  def on_success(self, data):\n",
        "\n",
        "    if 'extended_tweet' in data:\n",
        "        \n",
        "\n",
        "        text = data['extended_tweet']['full_text']\n",
        "        clean_source = data['source']\n",
        "        print(clean_source)\n",
        "        text_prep = preprocessor.process(text)\n",
        "        text_vectorized = classifier.vectorize(text_prep)\n",
        "        prediction = classifier.predict(text_vectorized)\n",
        "        class_prediction = (prediction > 0.5).astype(\"int32\")\n",
        "        if class_prediction.any() == 1:\n",
        "            text, chars, char_indices, indices_char, maxlen, sentences = generative.get_info(text_prep)\n",
        "            tweets = generative.generate_tweets(text, char_indices, indices_char,chars, maxlen)\n",
        "        \n",
        "        #stream.disconnect()\n",
        "        #time.sleep(4)\n",
        "\n",
        "        \n",
        "    \n",
        "    def on_error(self, status_code, data):\n",
        "        print(status_code)\n",
        "        return False"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftMgBa3JYqGR"
      },
      "source": [
        "twitter = Twython(\n",
        "    APIKey,\n",
        "    APIKeySecret,\n",
        "    AccesToken,\n",
        "    AccessTokenSecret\n",
        ")"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-vcBIcOCCkW",
        "outputId": "6f1b8b4d-ee8e-468a-88e0-ee141aa82e65"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KhPFWsTbv32Q",
        "outputId": "87cd03ba-8556-45a9-b312-b1c63cb679fe"
      },
      "source": [
        "stream = MyStreamer(\n",
        "    APIKey,\n",
        "    APIKeySecret,\n",
        "    AccesToken,\n",
        "    AccessTokenSecret\n",
        ")\n",
        "stream.statuses.filter(track='#covid19', language = \"en\", mode=\"extended\")"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>\n",
            "Tweet no. 001\n",
            "=============\n",
            "Generating with seed:\n",
            "an promote million network direct messag\n",
            "________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "an promote million network direct messag south health astim covid variant vaccin covid quebar call geat support ectin covid vaccin work trin\n",
            "\n",
            "Tweet no. 002\n",
            "=============\n",
            "Generating with seed:\n",
            "b onlyfan promote million network direct\n",
            "________________________________________\n",
            "b onlyfan promote million network direct alch alde covid vaccin work tring\n",
            "850,trank covid eteermon pay quercit incl covid covidvari covid c\n",
            "\n",
            "Tweet no. 003\n",
            "=============\n",
            "Generating with seed:\n",
            "e want sob onlyfan promote million netwo\n",
            "________________________________________\n",
            "e want sob onlyfan promote million networ covid vaccin covid\n",
            "293,seen get kit etherf boyth pandem sife covid vaccin work tring\n",
            "850,trank plo\n",
            "\n",
            "Tweet no. 004\n",
            "=============\n",
            "Generating with seed:\n",
            "n promote million network direct message\n",
            "________________________________________\n",
            "n promote million network direct message shit health aspirit realli conter covid variant vaccin covid\n",
            "292,covid provid test mote covid eteer\n",
            "\n",
            "Tweet no. 005\n",
            "=============\n",
            "Generating with seed:\n",
            "etwork direct message us safe could happ\n",
            "________________________________________\n",
            "etwork direct message us safe could happ need everi dichang make swear\n",
            "272,compleas fain vaccin mort aloon fand plac covid hoaper call gul s\n",
            "\n",
            "Tweet no. 006\n",
            "=============\n",
            "Generating with seed:\n",
            "work direct message us safe could happyb\n",
            "________________________________________\n",
            "work direct message us safe could happybli covid\n",
            "251,thing want th pport suse relest strond work thint support ectinali comid ast wart alod \n",
            "\n",
            "Tweet no. 007\n",
            "=============\n",
            "Generating with seed:\n",
            "mote million network direct message us s\n",
            "________________________________________\n",
            "mote million network direct message us sideamstay careen covid emert omicronvari ustay covid variant vaccin covid\n",
            "292,covid provid test mote\n",
            "\n",
            "Tweet no. 008\n",
            "=============\n",
            "Generating with seed:\n",
            "ork direct message us safe could happybi\n",
            "________________________________________\n",
            "ork direct message us safe could happybianalica\n",
            "202,nov updat thank share latest child teen covid hospit data\n",
            "306,wovid want vaccin sure soi\n",
            "\n",
            "Tweet no. 009\n",
            "=============\n",
            "Generating with seed:\n",
            "fan promote million network direct messa\n",
            "________________________________________\n",
            "fan promote million network direct messan alcohob go holld worder pandem afe covid variant hoaldhater covid _\n",
            "199,peet least mandat activ co\n",
            "\n",
            "Tweet no. 010\n",
            "=============\n",
            "Generating with seed:\n",
            "ct message us safe could happybirthdayji\n",
            "________________________________________\n",
            "ct message us safe could happybirthdayji vaccin covid\n",
            "293,sount test hotpit data\n",
            "306,wovid want vaccin sure soint covid variant vaccin covid\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TwythonAuthError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTwythonAuthError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-156-48724a6ea14f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mAccessTokenSecret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatuses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#covid19'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"extended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twython/streaming/types.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://stream.twitter.com/%s/statuses/filter.json'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m               \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twython/streaming/api.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, url, method, params)\u001b[0m\n\u001b[1;32m    150\u001b[0m                                       not valid JSON.')\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_success\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0mmessage_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mmessage_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-153-e161f90da641>\u001b[0m in \u001b[0;36mon_success\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclass_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#stream.disconnect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-150-6707192d547e>\u001b[0m in \u001b[0;36mgenerate_tweets\u001b[0;34m(self, corpus, char_to_idx, idx_to_char, chars, maxlen, n_tweets, verbose)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtwitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twython/endpoints.py\u001b[0m in \u001b[0;36mupdate_status\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'statuses/update'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mretweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twython/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, endpoint, params, version, json_encoded)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1.1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_encoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;34m\"\"\"Shortcut for POST requests via :class:`request`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_encoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1.1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_encoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twython/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, endpoint, method, params, version, json_encoded)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         content = self._request(url, method=method, params=params,\n\u001b[0;32m--> 273\u001b[0;31m                                 api_call=url, json_encoded=json_encoded)\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twython/api.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, url, method, params, api_call, json_encoded)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 retry_after=response.headers.get('X-Rate-Limit-Reset'))\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTwythonAuthError\u001b[0m: Twitter API returned a 401 (Unauthorized), An error occurred processing your request."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhfHJIlzYqGS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}